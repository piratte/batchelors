\chapter{Related works}

For comparison we can look for other similar systems as DIRAC. The DFC was made by the LHCb experiment collaboration,
its needs are similar to those of other LHC experiments so the ATLAS Distributed Data Management system and the AliEn 
file catalog would make a good comparison.


\section{ATLAS DDM}
The ATLAS experiment at LHC is a general purpose particle detector designed to investigate physics at the energy
frontier. The output data-rate, already reduced by the online trigger is  200-400Hz. Even with this reduction 
ATLAS records a huge amount of data â€“ more than 8PB of raw collision data have been taken since the LHC
started running. After processing at ATLAS Tier-0s, the data is registered in the ATLAS Distributed Data Management
system (DDM) \cite{ATLASDDM1}. Without deeply studying the implementation of it I would like to take a look 
at its main concepts.

Although a basic unit of data in the ATLAS DDM is a file the basic operational unit is a dataset: they may be
transferred to grid sites, whereas single files may not. There is also one more layer of aggragation called the 
container, which is a collection of datasets. Datasets may overlap and in practice they do so in a 
hierarchical manner: production will use small datasets, referring to a few jobs processed at a site. These datasets
are then aggregated into the main dataset, which refers to a particular task in the ATLAS
production system. Then these main datasets may be added to a container, where the output of several 
similar simulation tasks is gathered. Datasets may be opened (new files can be added to them), closed (no new files
may be added, but could have versions which differ in file content) and frozen (no new files may be added and no 
new versions created). 

The responsibilities of the ATLAS DDM cover data registration, transfers between sites, file deletion, dataset 
consistency ensuring (manage file loses on sites), enforcing ATLAS Computing model policies and monitoring. The
current implementation is called Rucio (previous was DQ2).


\subsection{Rucio}
In this implementation files, datasets and containers follow an identical naming scheme which is composed of the
scope and a name, called a data identifier \cite{ATLAS-Rucio}. Scopes are used to separate production and individual
users. Metadata associated with a data identifier is represented using attributes (e.g. for file its availability,
for dataset whether it is opened, closed or frozen,...) which are key-value pairs. The set of available attributes 
is restricted. Some metadata attributes are user settable, e.g. physics attributes (run number). Non user settable 
metadata include system attributes (size, creation time,...). For datasets and containers it is possible that the 
value of metadata is a function of the metadata of its constituents, e.g. the total size is the sum of the
sizes of the constituents.

A Rucio Storage Element (RSE) is a repository for physical files. It is the smallest unit of
storage space addressable within Rucio. It has an unique identifier and a set of properties (supported protocols,
storage type, physical space,...). Physical files stored on RSEs are identified by their Physical File Name (PFN).
The PFN is a fully qualified path identifying a replica of a file. The mapping between the file identifier and
the PFN is a deterministic function of the identifier, RSE and protocol. Replica management in Rucio is based on
replication rules defined on data identifier sets. A replication rule is owned by an account and defines the 
minimum number of replicas to be available on a list of RSEs.

\section{AliEn}

AliEn (ALICE Environment) is a Grid framework developed by the ALICE
Collaboration and used in production for more than 14 years. Since 2005, AliEn
has been used for both data production and user analysis  \cite{AliEn2}. 

The AliEn File Catalogue is one of the key components in the AliEn system. It
provides the mapping between Logical File Names (LFNs) visible to the end user
and one or more Physical File Names (PFNs) which describe the physical location
of the file by identifying the name of a storage element and the path to the
local file  The LFNs are manipulated by users, one LFN can point to more PFNs.
To prevent duplicate entries, each LFN is associated with a Globally Unique
IDentifier (GUID).
The interface of the catalogue is a hierarchical structure of directories and
files, similar to a UNIX file system. The directories and files in the File
Catalogue have Unix like privileges.
It also extends the familiar file system paradigm to include information about
running processes in the system (in analogy to the /proc directory on Linux
systems). Each job inserted into AliEn Task Queue gets a unique id and a
corresponding /proc/id directory where it can register temporary files,
standard input and output as well as all job products \cite{AliEn1}.
The File Catalogue is designed to allow each directory node in the hierarchy to
be supported by different database engines, possibly running on different hosts
and even having different internal table structures optimized for a particular
directory branch. 
From user point of view, the catalogue behaves as a single entity, internally
it is divided between the LFN and GUID catalogues, which are kept in separate
databases. Every LFN database has an index table used to locate the files and
table that contains all the information about the concrete LFN (owner, group,
size...).
In addition, there could be some user-defined tables containing metadata.
AliEn supports file collections.  A file collection is a user defined list of
entries, which can be ether another collections, LFNs or GUIDs. The collection
looks like a normal file in the file catalogue, its size is the sum of sizes of
all its components. If a command is executed on a collection, it will be
executed on each entry of that collection (e.g. if a user issues a command to
replicate a collection, all the files of that collection will be replicated).

\section{Main differences?}