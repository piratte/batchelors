\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\section*{The Worldwide LHC Computing Grid}
On the edge of our millennium scientists working on the Large Hadron Collider (LHC) were expecting enormous volumes 
of data, larger then any single computing center within the LHC collaboration could handle, so the concept of 
distributed data management was conceived. In 2001 the CERN\footnote{European Organization for Nuclear Research 
(name derived from Conseil Européen pour la Recherche Nucléaire) -- European research organization that operates 
the largest particle physics laboratory in the world.} Council approved the start of an international 
collaborative project that consists of a grid-based computer network infrastructure, the Worldwide LHC Computing 
Grid (WLCG)~\cite{happyBday}. 

The WLCG has a hierarchical architecture, where participating sites are categorized according to the resources and 
services they provide into four importance levels called Tiers. Each Tier is represented by a single or 
distributed computing and storage cluster and provides a specific set of services. The largest center, CERN data 
center or Tier-0, provides the  permanent storage of experimental data and makes the data available for the WLCG 
processing. Although it provides less than 20\% of the WLCG computing capacity, the role of CERN is unique in 
keeping one copy of the data from all experiments and for performing the first pass of the data reconstruction. 
When LHC is not running, Tier-0 provides resources for re-processing of the raw experimental data and eventually 
for simulation campaigns. 

\begin{wrapfigure}{L}{0.6\textwidth}
\centering
\includegraphics[width=0.55\textwidth]{Tiers.pdf}
\caption{The WLCG Tier-1 centers with CERN Tier-0 in the middle}
\label{fig:WLCG}
\end{wrapfigure}

Another copy is passed to one of the Tier-1 centers. Tier-1s are huge computing centers located in Europe, Canada, USA 
and Taipei. They provide non-stop support for the Grid, store a share of raw data, perform reprocessing and store 
its output. They are connected to CERN with dedicated high-bandwidth optical-fiber links. Then there are more than 
160 Tier-2 centers all around the world. Their role is mainly to run simulation campaigns and end-user analysis. 
Tier-3 centers are small local computing clusters at universities or research institutes and even individual 
PCs~\cite{TGrid}.

\section*{Grid Middleware}

The operation and functionality of WLCG, as well as other Grid systems, is enabled by specific software packages 
and protocols, so-called Grid middleware. This manages the basic domains of the Grid functions: job management, 
data management, security and information services \cite{GriCom}. The term middleware reflects the specific role 
of this software packages and protocols system: it is a layer between the application area for solving users tasks 
and the resource area consisting of basic fabric and connectivity layer. 

The vast variety of requirements and needs of the user communities from the four LHC experiments is impossible to 
meet with only one set of middleware components. Consequently, each experiment user group started developing its 
own set of tools, which meet their needs. For example AliEn is a middleware solution made by the 
ALICE\footnote{One of the experiments hosted at CERN} experiment collaboration and DIRAC was developed by the 
LHCb\footnote{Another experiment hosted at CERN} collaboration. Along with some packages from the WLCG-middleware 
they include some additional specific packages and provide complete framework for data processing according to the 
individual experiments' computing models.

\section*{DIRAC}

The DIRAC\footnote{The Distributed Infrastructure with Remote Agent Control} middleware was developed to satisfy
the needs of not only the LHCb collaboration developing it, but also to enable other smaller experiments to use
it as their middleware solution. This was achieved by concentrating on modular architecture, which helps with
adding new features or modifying the systems behavior according to individual experiments needs. 

DIRAC is constructed from loosely coupled systems where each system manages one part of its functionality. This 
thesis concentrates on the DIRAC File Catalog, which is part of the Data Management system. This particular system 
is responsible data managing tasks across a wide range of distributed storage elements. It also enables users to 
quickly find and use their files. This is the goal of the File Catalog. It is accomplished by maintaining a 
directory structure with a similar interface as UNIX shell and enabling users to define their own metadata
and use them to search for files.

\section*{Task of the Thesis}

The first task of this thesis is to upgrade the DIRAC File Catalog by 
\begin{itemize}
\item adding a new module for dataset support enabling users to bundle their files, based on a metadata search 
(a~metaquery) into a single object,
\item implementing a class to encapsulate all the methods handling metaquery as well as to extend its 
functionality by adding normalization and optimization procedures.
\end{itemize}

%\noindent 
The second task is to test the hypothesis that storing the user defined metadata in a suitable NoSQL database 
would improve metaquery performance. If the tests prove that hypothesis, the task is to extend the code of DIRAC 
to incorporate the database in the File Catalog making a prototype, that can be then evaluated by the DIRAC 
collaboration.

\section*{Structure of the Thesis}

In chapter \ref{chap:DIRAC} the DIRAC middleware will be introduced with focus on the data management part and the 
file catalog. Afterwards DIRAC will be compared to two other middleware solutions in chapter \ref{chap:relwork}, 
more specifically ATLAS Distributed Data Management system and ALICE Environment framework.

In the next two chapters the contribution of this project to DIRACs code will be presented. Chapter \ref{chap:MQ} 
is about the new MetaQuery class and chapter \ref{chap:Dataset} refers about the Dataset Manager.

In the last part several NoSQL databases are tested in order to pick the one that would be the best for storing
file metadata (chapter \ref{chap:databases}) and in chapter \ref{chap:NoSQL} a module created for integrating that 
database is described. 

Finally chapter \ref{chap:user} provides user documentation to all the commands used to interact with the CLI of 
the DFC used to control any of the parts that were changed by this project. The last chapter
provides conclusion as well as evaluation of the test results.