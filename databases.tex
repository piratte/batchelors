\chapter{FileCatalog connected to a NoSQL database}
\label{chap:databases}
One of the main goals of this project was to test whether connecting the 
file catalog, more specifically its metadata part, to a NoSQL database would 
improve the feedback speed of the service thus making it more pleasant to use
or make it easier to implement and maintain. The new database had to satisfy the
following conditions in order to be connectable to DIRAC and deployable in
the computing centers.

\begin{itemize}
\item There has to be a free-ware version which could DIRAC use.
\item The database has to have a python interface or client so it would be easy
to incorporate into DIRACs code.
\end{itemize}

The characteristics of the data itself add some restrictions. The database should be
optimized for search speed. When combined with the current metadata implementation, we
get two different types of data retrieval. For directories 
the database has to be able to get all the metadata associated with a directory
identified by an ID. On contrary the files are fetched based on the terms in the metaquery so
all the querying techniques the metaquery could use have to be possible, including
range queries. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{dataTyp.pdf}
\caption{Theoretical data scheme in the metadata catalog}
\label{fig:theoDataScheme}
\end{figure}


\section{Apache Cassandra}
Apache Cassandra is a distributed database for managing large amounts of structured data 
across many commodity servers, while providing highly available service and no single point 
of failure \cite{cassandra}. Cassandra's data model is a
partitioned row store, rows are organized into tables. Each row
has an altering number of columns, which are essentially a
key-value pair, and is identified by a unique ID. Adding that Cassandra features
its own query language CQL (Cassandra Query Language) which resembles the standard SQL 
used by all relational databases, it looks like a perfect candidate for the metadata catalog.

\subsection{Developing a data model for Cassandra}

Although CQL makes it look like a relational database, the developer has to bare in mind, that
Cassandra is different. The first scheme used the altering number of columns and organized the 
data in tables, where each file or directory had its own row with metanames being the column
names and values being the column values (this model greatly resembles the theoretical data 
scheme in figure \ref{fig:theoDataScheme}) with secondary indexes over column values so that 
the query would be possible. Although the similarity with SQL would suggest otherwise, 
it turned out that this kind of indexing does not support range queries so this data-model
had to be abandoned. 

After understanding more thoroughly how Cassandra works, another data-model was introduced 
using most of Cassandra specific features. The key properties Cassandra guaranties are that 
rows are not divided across multiple nodes in the cluster and column keys inside the row
are sorted. Based on these two characteristics a functioning data model was created. For  
directories row IDs are mapped on directory IDs and the model is similar to the previous one. 
Retrieving a row with given row ID is one of supported operations.
For files each metaname has its row, column names are meta values and column values are sets of 
file IDs of files having this metaname and value associated with them. 

\begin{table}[h]
\centering
\label{tab:fileMeta}
\begin{tabular}{|l|l|l|l|ll}
\hline
\multirow{2}{*}{Metaname A} & Value 1      & Value 2      & Value 3      & \multicolumn{1}{l|}{Value 4}      & \multicolumn{1}{l|}{Value 5}      \\ \cline{2-6}
                            & \{id,id,..\} & \{id,id,..\} & \{id,id,..\} & \multicolumn{1}{l|}{\{id,id,..\}} & \multicolumn{1}{l|}{\{id,id,..\}} \\ \hline
\multirow{2}{*}{Metaname B} & Value 1      & Value 2      & Value 3      &                                   &                                   \\ \cline{2-4}
                            & \{id,id,..\} & \{id,id,..\} & \{id,id,..\} &                                   &                                   \\ \cline{1-4}
\end{tabular}
\caption{File metadata}
\end{table}

The rows are grouped in tables by value type, because the algorithm used to sort the
column names is unified per table. There also is an index over fileID sets, 
so that retrieving all metadata for one specific file is possible, but the scheme is 
not optimized for this operation, because this is done only for the users information and
when setting new metadata.

\begin{listing}
\begin{minted}{sql}
CREATE TABLE file_int (
    metaname text,
    value int,
    fileid set<int>,
    PRIMARY KEY (metaname, value)
);
\end{minted}
\caption{Data structure described using CQL}
\end{listing}

In CQL this structure looks like a table with three columns and a compound primary key which
brings the main disadvantage of this approach: meta names can be queried only one at a time and
the result has to be then finalized in the DIRACs code. Because the expected structure of data could
suggest, that files satisfying only a part of the metaquery could be many, the idea of using 
Cassandra as the NoSQL database was dropped, because fetching multiple times a large number of 
files from the database and then doing a intersection in the python code is clearly not optimal. 

\begin{listing}
\begin{minted}{sql}
SELECT fileid 
	FROM file_int 
	WHERE metaname='metaInt1' AND value>3;
\end{minted}
\caption{Example query}
\end{listing}

\section{Document databases}

Both following databases are document-oriented and the data scheme is fairly similar in both
of them. A document-oriented database replaces the concept of a \textit{row} from the world of relational 
databases with a more dynamic and versatile \textit{document}. Allowing arrays and embedded documents the 
document-oriented databases provides de-normalization of the data and  of complex 
relationships in a single document. Also there are no predefined schemes, which helps with development 
and testing and in case of this project is essential because the number of associated metadata varies between 
files. The metadata are stored in a JSON structure with fields being meta names, indexes above properties and key 
mapped to the dir or file ID.

\begin{listing}
\begin{minted}{json}
{
	'id'       : id
	'metaInt1' : 1,
	'metaStr1' : 'qwerty',
	'metaInt3' : 123
}
\end{minted}
\caption{JSON structure used in document databases}
\end{listing}

Document databases were not the first pick during developing this project, because the idea of
storing metadata in a JSON structure and then building indexes above the properties is not as
familiar as Cassandras columns, but it turned out to be even easier to use.

\subsection{MongoDB}

Mongo is a open-source document-oriented database storing JSON files. As of November 2015, MongoDB is the fourth 
most popular type of database management system, and the most popular for document stores 
\footnote{Ranking database management systems according to their popularity: \url{http://db-engines.com/en/ranking}, ref. November 2015}. 
In MongoDB the document is the basic unit, documents are grouped into collections, which can be thought of as 
a table with a dynamic schema. Single instance of MongoDB can have multiple databases, each can host multiple 
collections. In collections documents are identified using a special field \texttt{\_id} which has to be unique
within a collection\cite{MongoBook} (this projects maps the file or directory ids from the file catalog 
to this id field) .

\subsubsection{Using MongoDB}

Running MongoDB is rather simple: on Scientific Linux it could be installed from a package with reasonable 
defaults (the only thing that was changed on the testing server was the data directory and listening on the 
universal IP address had to be enabled, default is only local host). MongoDB ships with its own shell based on
JavaScript the administrator can use to query and update data as well as perform administrative 
operations\footnote{\url{https://docs.mongodb.org/getting-started/shell/client/}}. This client is rather easy to
use and the commands used in it are very similar to those used by the python library, which also helps. The mongo
package also contains several tools for monitoring the database, which are handy when testing querying performance,
including the \texttt{mongotop}\footnote{\url{https://docs.mongodb.org/manual/reference/program/mongotop/}} 
and \texttt{mongostat}\footnote{\url{https://docs.mongodb.org/manual/reference/program/mongostat/}}, and several 
more helping the administrator with e.g. dumping and restoring the database. For further 
debugging the log file provides a sufficient amount of information, when its verbosity is turned up to at least 3.
Overall the database is easy to use and administer thanks to many well documented utilities. 

There are two types
of configuration files supported by MongoDB: the older one in form of \texttt{<setting> = <value>} and the 
newer in Yaml format. The database ships with the older one (although it is not developed since version 2.4), but
most of the configuration documentation is in the newer one (and some of the new features can be only set in 
Yaml format) so the administrator should convert it manually before using the database since it could ease up
the usage and optional re-configuration later.
The major drawback when using this database is that every property has to be indexed manually, which not only 
takes time, but also consumes disk space (see %TODO reference to the chapter later). 
% When querying in a collection that does not exist, there is no error, just no result, which can be confusing

\subsection{Elasticsearch}

Elasticsearch (ES) is a real-time distributed opend-source analytics and search engine \cite{ESBook}, 
build on top of Apache Lucene\footnote{\url{https://lucene.apache.org/}}. Unlike very complex Lucern, 
ES features a simple RESTful API that makes it easy to use. Moreover it could be used not only 
for full-text search, but for real-time analytics or, which is important for this project, as a 
distributed document store, where \textit{every} field is indexed and search-able. Its' python 
client\footnote{\url{http://elasticsearch-py.readthedocs.org/en/master/}} provides a wrapper for the RESTful API
as well as some useful additional commands called helpers (e.g. for bulk loading data, the command 
\texttt{helpers.bulk(es, dataToLoad(size))} was used).

\subsubsection{Using ES}

As well as MongoDB, Elasticsearch ships in a package so downloading and installing it is very simple. The 
configuration file is in Yaml format. There is no need for any initial data structure,
the first data are simply inserted using the correct URL\footnote{\url{http://host:port/index/doc_type/doc_id}
is the URL the RESTful API uses, when using the python library \texttt{index, doc\_type}\texttt{doc\_id} are 
specified using the function arguments}. ES structures data in indexes and types (hence the \texttt{index} and
\texttt{doc\_type} in the command). An Elasticsearch cluster can contain multiple indexes, which can contain 
multiple types (there is a rough parallel with the world of relational databases, where indexes would be databases 
and types would correspond with tables). Unlike the relational databases ES can create the index and type with the 
first inserted document using its' default configuration.

% When a value of a field is set to a non-numeric value, a numeric_range query cannot be executed after that, even though all the currently set values are numeric

\section{Database tests}

For testing a single IMB iDataPlex dx340 server was used, equipped with 2x (Intel Xeon 5440 with 4 cores), 16GB
of RAM and 300GB SAS hard drive, where all the database files were saved. After several smaller datasets, 
the main testing data was generated trying to mimic the production data structure. There is expected to
be over 10 million files with approximately different 20 meta names. %TODO odkaz na prilohu s Jirkovym mailem 
The set generated had $10 000 000 - 1$ files with $1$ to $998$ metanames associated with them\footnote{ 
(the generator chose a randomly $1$ to $499$ integer metrics and the same number of string ones)}, which is more
then the production data would have, but gave the testing more data to stress the databases.
The metrics names were \texttt{test[Int|Str]NNN}, where \texttt{NNN} stands for a number. The
integer values were all picked from the interval $[1;1000]$ and the string values were one of the
words from the NATO phonetic alphabet\cite{NATO}, which lead to the easier composition of testing queries. 
Integer fields represent continuous data types and string fields represent discreet data types.

The data in form of two csv files with lines \texttt{id,metaname,value} were 63 and 59 GB big.

\subsection{Loading the big data}

Although once in production the user will not probably use the bulk loading functionality of the databases, in
this project that involved testing the databases on a large volume of data the loading of the data was one 
of the minor problems it had to tackle.

Elasticsearchs python interface features a bulk load function which when fed with a python generator type
loads the data in chunks of 500. Unfortunately it crashes on a TimeoutException from time to 
time making the loading a rather long procedure. 

MongoDB features a command \texttt{mongoimport} which can read a file of JSON objects and load
them all. Its' pace on the test data was approximately $150-200$ entries per second. Unluckily 
the script froze from time to time so loading the whole dataset took again several days. Once the data was
loaded, they could be back-uped using the utility \texttt{mongodump} and then again loaded using the 
command \texttt{mongorestore}. These two utilities ran without problems and where used when changing the 
storage engine.

\subsection{Testing query performance on a predefined sample}

The task of the database in this deployment will be to accept a query, execute the search and return all the 
ids of documents\footnote{Document ids are mapped on file ids, so this procedure is returning the ids of files
that satisfy the query} satisfying the query. To measure the results as precisely as possible, a list of
queries was generated. When testing one database, all other database services were stopped so that they do not
interfere with the test. To effectively search for data in MongoDB the document properties have to be 
indexed manually. Indexes were made on several integer fields (testInt1-8) and three string fields 
(testStr1,testStr2,testStr3). The time cost of building\footnote{for the default storage engine} the 
indexes and their space requirements are listed in table \ref{tab:indexBuildTimes}.

\begin{table}[h]
\centering
\label{tab:indexBuildTimes}
\begin{tabular}{|l|l|l|l|l|}
\hline
Field    & Time     & Disk Space (MMAP) & Disk Space (WT) \\ \hline
testInt1 & 00:20:50 & 211 390           & 46 785          \\ \hline
testInt2 & 00:20:00 & 211 407           & 46 789          \\ \hline
testInt3 & 00:19:59 & 211 390           & 46 785          \\ \hline
testInt4 & 00:20:12 & 211 415           & 46 789          \\ \hline
testInt5 & 00:19:58 & 211 390           & 46 785          \\ \hline
testStr1 & 00:20:06 & 203 043           & 44 351          \\ \hline
testStr2 & 00:20:13 & 203 026           & 44 351          \\ \hline
testStr3 & 00:20:51 & 203 043           & 44 356          \\ \hline
testInt6 & 00:21:03 & 211 407           & 46 789          \\ \hline
testInt7 & 00:19:57 & 211 399           & 46 785          \\ \hline
testInt8 & 00:19:58 & 211 390           & 46 785          \\ \hline
\end{tabular}
\caption{The time and storage requirements for indexes used in MongoDB. The indexes were built while using
the MMAP storage engine. Sizes are in MB.}
\end{table}

Elasticsearch does not have a similar limitation, however the queries were kept
the same so that the comparison is as fair as possible. 

The queries were generated randomly combining the integer and string properties. The number of hits was not 
considered while generating the queries. All the queries used are in appendix A (??)%TODO add queries to an appendix?

The program testing the performance is a python script running on a personal computer in the same LAN as the 
database machine (similarly to the expected deployment of the service). The measured time is the interval between 
the moment the query was submitted, to the moment the results were extracted to a python list. Nor the 
preparation of the query, nor the printing of the results were counted in the final time. 

For Elasticsearch the cache of the index was flushed after every query to keep the results consistent (although as 
figure \ref{fig:EScache} suggests, flushing the cache does not make a notable difference for bigger data volumes). 

\begin{figure}[t]
	\centering
	\input{img/ES_comparison.tex}
%	\includegraphics[width=\textwidth]{ES_comparison.pdf}
	\caption{Query times for Elasticsearch comparing performance with and without dumping depending on number of 
	hits (logarithmic scale)}
	\label{fig:EScache}
\end{figure}

The original MongoDB instance was using the default storage engine used by versions up to 3.0\footnote{MMAPv1 
Storage Engine based on memory mapped files}. There is also a new storage engine WiredTiger\footnote{
\url{https://docs.mongodb.org/manual/core/wiredtiger/}} available so the performances of those two were compared.
Moreover the new engine adds the option of choosing the compression strategy, the comparison of disk space usage 
can be seen in table \ref{tab:MongoComp} and query performance in figure \ref{fig:MDBcomparison}.

\begin{table}[]
\centering
\label{tab:MongoComp}
\begin{tabular}{|l|r|r|}
\hline
Storage engine                      & \multicolumn{1}{l|}{Total database size} & \multicolumn{1}{l|}{Total index size} \\ \hline
MMAPv1                              & 136 161                                  & 2 579                                 \\ \hline
WiredTiger with zlib compression    & 22 788                                   & 615                                   \\ \hline
WiredTiger with default compression & 42 249                                   & 616                                   \\ \hline
\end{tabular}
\caption{Disk usage comparison of MongoDBs storage engines. Note the size of the indexes does not rely on the
compression used. All sizes are in MB. For comparison the size of the data in Elasticsearch is 59 716MB.}
\end{table}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{MDB_comparison.pdf}
	\caption{Query times for MongoDB comparing performance with different storage engines and compression 
	options (logarithmic scale)}
	\label{fig:MDBcomparison}
\end{figure}

The conclusion is that if the administrator does not have a critically low amount of disk space available, MongoDB
works best with the new WiredTiger storage engine with default compression. In figure \ref{fig:DBscomparison} we 
can see the comparison of performance on the sample queries between the WiredTiger configuration of MongoDB and 
Elasticsearch. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{DBs_comparison.pdf}
	\caption{Query times for MongoDB comparing performance with different storage engines and compression 
	options (logarithmic scale)}
	\label{fig:DBscomparison}
\end{figure}

The queries on MongoDB are taking much more time than on Elasticsearch. Luckily MongoDB provides
a tool for explaining query execution which comes in very useful when trying o investigate efficiency problems
like this one. When trying to explain the query %TODO insert query
the output of the command can be seen in listing %TODO add listing and reference
As we can see, MongoDB tries all the indexes associated with one of the queried properties, then picks one and 
based on just it performs the search. The search results are then filtered so that the returned fields satisfy the
other conditions of the input query. To test the best possible performance, special compound indexes were created 
for each query and the performance was tested using these indexes. 

%TODO create the indexes, test the queries and post the results

\subsection{The current deployment performance}

Currently the database back-end to all the services in DIRAC, where a database is needed including the File Catalog
and its' metadata part, is relying on MySQL. The table scheme of the metadata part of the DFC can be seen in figure
\ref{fig:DFCUML}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{DFCDB.pdf}
	\caption{The File Catalog part of the database scheme. The tables on the left are created implicitly with the
	rest of the scheme, they are used  for keeping track of the types of metafields and un-indexed  metadata 
	values. For each new indexed field a new table is created, in this scheme there are 3 indexed file metafields
	(X, Y, Z) and three indexed directory metafields (A,B,C).}
	\label{fig:DFCUML}
\end{figure}

When